{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalisation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_norm (data, method='log'):\n",
    "    if method == 'log':\n",
    "        tmp_log = data[data != 0]\n",
    "        logOS = np.nanmedian(tmp_log)\n",
    "        Data_log = np.log(data+logOS)\n",
    "        Data_norm = Data_log\n",
    "    \n",
    "    if method == 'tic':\n",
    "        Data_TIC=np.zeros((data.shape[0],data.shape[1]))\n",
    "        for i in range (0,data.shape[0]):\n",
    "            try:\n",
    "                if data.values[i].all == 0:\n",
    "                    Data_TIC[i] = 0\n",
    "                else:\n",
    "                    Data_TIC[i] = data.values[i]/np.sum(data.values[i])\n",
    "            except:\n",
    "                if data[i].all == 0:\n",
    "                    Data_TIC[i] = 0\n",
    "                else:\n",
    "                    Data_TIC[i] = data[i]/np.sum(data[i])\n",
    "        Data_norm = Data_TIC\n",
    "        \n",
    "    if method == 'minmax':\n",
    "        Data_mm=np.zeros((data.shape[0],data.shape[1]))\n",
    "        for i in range (0,data.shape[0]):\n",
    "            try:\n",
    "                if data.values[i].all == 0:\n",
    "                    Data_mm[i] = 0\n",
    "                else:\n",
    "                    Data_mm[i] = (data.values[i]-np.min(data.values[i]))/np.max(data.values[i])\n",
    "            except:\n",
    "                if data.values[i].all == 0:\n",
    "                    Data_mm[i] = 0\n",
    "                else:\n",
    "                    Data_mm[i] = (data[i]-np.min(data[i]))/np.max(data[i])\n",
    "        Data_norm = Data_mm\n",
    "        \n",
    "    if method == 'snv':\n",
    "        Data_snv=StandardScaler().fit_transform(data)\n",
    "        Data_norm = Data_snv\n",
    "    \n",
    "    Data_norm = np.nan_to_num(Data_norm)\n",
    "    return(Data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for model training + CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv (data, method, cvsplit, label, group, weights = None, scoring = None):\n",
    "    models = []\n",
    "\n",
    "    if method == 'LR':\n",
    "        if weights != None:\n",
    "            models.append((LogisticRegression(solver='liblinear',class_weight = weights)))\n",
    "        else:\n",
    "            models.append((LogisticRegression(solver='liblinear')))\n",
    "\n",
    "    if method == 'LDA':\n",
    "        models.append((LinearDiscriminantAnalysis()))\n",
    "    if method == 'KNN':\n",
    "        models.append((KNeighborsClassifier()))\n",
    "    if method == 'CART':\n",
    "        models.append((DecisionTreeClassifier()))\n",
    "    if method == 'NB':\n",
    "        models.append((GaussianNB()))\n",
    "    if method == 'SVM':\n",
    "        if weights != None:\n",
    "            models.append((SVC(probability=True, kernel = 'linear',\n",
    "                                      decision_function_shape='ovo',class_weight = weights)))\n",
    "        else:\n",
    "            models.append((SVC(probability=True, kernel = 'linear',\n",
    "                                      decision_function_shape='ovo')))\n",
    "    if method == 'RF':\n",
    "        numIterations = 10;\n",
    "        num_trees = 150\n",
    "        max_features = 10\n",
    "        n_jobs = None\n",
    "#         n_jobs = -1\n",
    "        \n",
    "        if weights != None:\n",
    "            model = ((RandomForestClassifier(n_estimators= num_trees, max_features=max_features, \n",
    "                                             n_jobs = n_jobs,class_weight = weights)))\n",
    "        else:\n",
    "            model = ((RandomForestClassifier(n_estimators= num_trees, max_features=max_features, \n",
    "                                             n_jobs = n_jobs)))\n",
    "\n",
    "\n",
    "    # Store results in a pandas dataframe\n",
    "    results=pd.DataFrame(index=range(0,len(data)), columns=[method])\n",
    "    n=1\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if cvsplit == 'loo':\n",
    "        cvFold = logo.split(data,label,groups=group)\n",
    "    if cvsplit == 'kfold':\n",
    "        skf = StratifiedKFold(n_splits=10)\n",
    "        cvFold = skf.split(data, group)\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "#         print(i)\n",
    "        # only RF needs to be iterated as everything else deterministic\n",
    "        if method == 'RF':\n",
    "            RF_results=[]\n",
    "            for n in range(0,numIterations):\n",
    "                cvFold = logo.split(data,label,groups=group)\n",
    "                ite = cross_val_predict(model, data, label, cv=cvFold)\n",
    "                RF_results.append(ite)\n",
    "                msg = \"Method %s iteration %d \\r\" % (model, n)\n",
    "                print(msg,end=\"\")\n",
    "#             results.iloc[:,i]=(RF_results/numIterations).sum(axis=1)\n",
    "                results = RF_results\n",
    "        else:\n",
    "            results.iloc[:,i]=cross_val_predict(model, data, label, cv=cvFold)\n",
    "            # Status message\n",
    "            msg = \"Method %s iteration %d \\r\" % (model, i)\n",
    "            print(msg,end=\"\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print()\n",
    "    print('that took '+str(elapsed_time)+' seconds')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV behaviour visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_binarise (label):\n",
    "    classes = np.unique(label)\n",
    "    new_label = label\n",
    "    for i in range (0,len(classes)):\n",
    "        class_i = classes[i]\n",
    "        new_label[new_label==class_i] = i\n",
    "    return new_label\n",
    "\n",
    "\n",
    "def plot_cv(cv, X, old_label, old_group, n_splits, lw=10):\n",
    "    cmap_data = plt.cm.Paired\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    fig, ax = plt.subplots()\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X,old_group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(\n",
    "            range(len(indices)),\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=cmap_cv,\n",
    "            vmin=-0.2,\n",
    "            vmax=1.2,\n",
    "        )\n",
    "    new_label = multilabel_binarise (old_label)\n",
    "    new_group = multilabel_binarise (old_group)\n",
    "    ax.scatter(\n",
    "            range(len(X)), [ii + 1.5] * len(X), c=new_label, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "        )\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 2.5] * len(X), c=new_group, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "    )\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n",
    "    ax.set(\n",
    "        yticks=np.arange(n_splits + 2) + 0.5,\n",
    "        yticklabels=yticklabels,\n",
    "        xlabel=\"Sample index\",\n",
    "        ylabel=\"CV iteration\",\n",
    "    )\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cm (cv_results, label, perc=False):\n",
    "    results=pd.DataFrame(index=range(0,len(label)), columns=['LOOCV results','labels'])\n",
    "    results.iloc[:,0]=cv_results\n",
    "    results.iloc[:,1]=label\n",
    "    labelnames = set(label)\n",
    "    labelnames = list(labelnames)\n",
    "    dims = len(labelnames)\n",
    "    \n",
    "    #calculate diagonals for TPs & TNs\n",
    "    TRUE = []\n",
    "#     TN = []\n",
    "    for i in range (0,dims):\n",
    "        condition1=(results['LOOCV results']==1) & (results['labels'] == labelnames[i])\n",
    "        dum=results[condition1]\n",
    "        TRUE.append(len(dum))\n",
    "    \n",
    "    FALSE = []\n",
    "    #calculate other elements\n",
    "    for i in range (0,dims):\n",
    "        condition2=(results['LOOCV results']==0) & (results['labels'] == labelnames[i])\n",
    "        dum=results[condition2]\n",
    "        FALSE.append(len(dum))\n",
    "    c_matrix=np.zeros((dims,dims))\n",
    "    for i in range (0, dims):\n",
    "        for j in range (0, dims):\n",
    "            if i == j:\n",
    "                c_matrix[i,j]= TRUE [i]\n",
    "            else:\n",
    "                c_matrix[i,j]= FALSE [i]\n",
    "    if perc == True:\n",
    "        c_matrix = c_matrix/sum(c_matrix)\n",
    "    return c_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to calculate F1/pre/acc/sen/spe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_FPASS (cm,cm_label):\n",
    "    dims = len(cm_label)\n",
    "    results=pd.DataFrame(index=range(0,dims+1), columns=['class','accuracy','sensitivity','specificity','precision','F1'])\n",
    "    results.iloc[:-1,0]=cm_label\n",
    "\n",
    "    \n",
    "    for i in range(dims):\n",
    "        TP = cm[i][i]\n",
    "        FN = np.sum(cm[i,:])-TP\n",
    "        FP = np.sum(cm[:,i])-TP\n",
    "        TN = np.sum(np.sum(cm))-TP-FP-FN\n",
    "        results.iloc[i,2] = (TP)/(TP+FN)\n",
    "        results.iloc[i,3] = (TN)/(TN+FP)\n",
    "        results.iloc[i,4] = (TP)/(TP+FP)\n",
    "        results.iloc[i,5] = 2*(TP)/(2*TP+FP+FN)\n",
    "        results.iloc[i,1] = (results.iloc[i,2]+results.iloc[i,3])/2\n",
    "    \n",
    "    results.iloc[i+1,0]='Average'\n",
    "    results.iloc[i+1,1] = np.mean(results.iloc[:,1])\n",
    "    results.iloc[i+1,2] = np.mean(results.iloc[:,2])\n",
    "    results.iloc[i+1,3] = np.mean(results.iloc[:,3])\n",
    "    results.iloc[i+1,4] = np.mean(results.iloc[:,4])\n",
    "    results.iloc[i+1,5] = np.mean(results.iloc[:,5])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to do ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ROC (X,Y, method = None, cvsplit = 'kfold', n_splits = 10, group = None, plotwhich = 'macro', \n",
    "            saved_model = False, saveprobs = False):    \n",
    "    if method == 'LR':\n",
    "        if weights != None:\n",
    "            model = ((LogisticRegression(solver='liblinear',class_weight = weights)))\n",
    "        else:\n",
    "            model = ((LogisticRegression(solver='liblinear')))\n",
    "\n",
    "    if method == 'LDA':\n",
    "        model = ((LinearDiscriminantAnalysis()))\n",
    "    if method == 'KNN':\n",
    "        model = ((KNeighborsClassifier()))\n",
    "    if method == 'CART':\n",
    "        model = ((DecisionTreeClassifier()))\n",
    "    if method == 'NB':\n",
    "        model = ((GaussianNB()))\n",
    "    if method == 'SVM':\n",
    "        if weights != None:\n",
    "            model = ((SVC(probability=True, kernel = 'linear',\n",
    "                                      decision_function_shape='ovo',class_weight = weights)))\n",
    "        else:\n",
    "            model = ((SVC(probability=True, kernel = 'linear',\n",
    "                                      decision_function_shape='ovo')))\n",
    "    if method == 'RF':\n",
    "        numIterations = 10;\n",
    "        num_trees = 150\n",
    "        max_features = 10\n",
    "        n_jobs = -1\n",
    "        \n",
    "        if weights != None:\n",
    "            model = ((RandomForestClassifier(n_estimators= num_trees, max_features=max_features, \n",
    "                                             n_jobs = n_jobs,class_weight = weights)))\n",
    "        else:\n",
    "            model = ((RandomForestClassifier(n_estimators= num_trees, max_features=max_features, \n",
    "                                             n_jobs = n_jobs)))\n",
    "    \n",
    "    if cvsplit == 'loo':\n",
    "        cv = LeaveOneOut()\n",
    "    if cvsplit == 'kfold':\n",
    "        cv = StratifiedKFold(n_splits=n_splits)\n",
    "    \n",
    "    cm_label = np.unique(Y)\n",
    "    if saved_model != False:\n",
    "        model = saved_model\n",
    "        all_probs = model.predict_proba(X)\n",
    "        all_y = Y\n",
    "    else:\n",
    "        all_y = []\n",
    "        all_probs=[]\n",
    "        if group is not None:\n",
    "            if cvsplit == 'kfold':\n",
    "                print('kfold')\n",
    "                for train, test in cv.split(X, group):\n",
    "                    all_y.append(Y[test])\n",
    "                    all_probs.append(model.fit(X[train], Y[train]).predict_proba(X[test]))\n",
    "            else:\n",
    "                print('LOGO')\n",
    "                for train, test in cv.split(X, group):\n",
    "                    all_y.append(Y[test])\n",
    "                    all_probs.append(model.fit(X[train], Y[train]).predict_proba(X[test]))\n",
    "        else:\n",
    "            for train, test in cv.split(X, Y):\n",
    "                all_y.append(Y[test])\n",
    "                all_probs.append(model.fit(X[train], Y[train]).predict_proba(X[test]))\n",
    "\n",
    "        all_y = np.array(all_y)\n",
    "        all_probs = np.array(all_probs)\n",
    "\n",
    "        if cvsplit == 'kfold':\n",
    "            n = randrange(n_splits)\n",
    "            all_probs_n = all_probs[n]\n",
    "            all_y_n = all_y[n]\n",
    "    #     all_probs_np = []\n",
    "    #     for i in range(len(X)):\n",
    "    #         all_probs_np.append(np.array(all_probs[i]))\n",
    "    n_classes = len(labelnames)\n",
    "    print('there are '+ str(n_classes)+' classes')\n",
    "\n",
    "\n",
    "    TPRs = dict()\n",
    "    FPRs = dict()\n",
    "    AUCs = dict()\n",
    "    for i in range(n_classes):\n",
    "        if cvsplit == 'loo':\n",
    "            FPRs[i], TPRs[i], _ = roc_curve(all_y,all_probs[:,0,i],pos_label=cm_label[i])\n",
    "#             FPRs[i], TPRs[i], _ = roc_curve(all_y,all_probs[i],pos_label=cm_label[i])\n",
    "        if cvsplit == 'kfold':\n",
    "            FPRs[i], TPRs[i], _ = roc_curve(all_y_n,all_probs_n[:,i],pos_label=cm_label[i])\n",
    "\n",
    "        AUCs[i] = auc(FPRs[i], TPRs[i])\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([FPRs[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, FPRs[i], TPRs[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "    mean_tpr[0] = 0.0\n",
    "\n",
    "    FPRs[\"macro\"] = all_fpr\n",
    "    TPRs[\"macro\"] = mean_tpr\n",
    "    AUCs[\"macro\"] = auc(FPRs[\"macro\"], TPRs[\"macro\"])\n",
    "    \n",
    "    # plot\n",
    "    sns.set_context(\"notebook\")\n",
    "    plt.figure(1, figsize=(10,10))\n",
    "    if plotwhich == 'macro':\n",
    "        plt.plot(FPRs[plotwhich], TPRs[plotwhich], lw=4, alpha=0.5, \n",
    "                 label='(%s) ROC %s  (AUC = %0.2f)' % (cvsplit, 'macro-averaged', AUCs[plotwhich]))\n",
    "    else:\n",
    "        plt.plot(FPRs[plotwhich], TPRs[plotwhich], lw=4, alpha=0.5, \n",
    "                 label='(%s) ROC for %s  (AUC = %0.2f)' % (cvsplit, cm_label[plotwhich], AUCs[plotwhich]))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', alpha=.8)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate',fontsize=20)\n",
    "    plt.ylabel('True Positive Rate',fontsize=20)\n",
    "    plt.title('Receiver Operating Characteristic (%s)' %method,fontsize=20)\n",
    "    leg = plt.legend(loc=\"lower right\")\n",
    "    leg_lines = leg.get_lines()\n",
    "    leg_texts = leg.get_texts()\n",
    "    plt.setp(leg_lines, linewidth=4)\n",
    "    plt.setp(leg_texts, fontsize='x-large')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    if saveprobs == True:\n",
    "        if cvsplit == 'loo':\n",
    "            probs_out = pd.DataFrame(data=all_probs[:,0,:],columns = labelnames)\n",
    "        if cvsplit == 'kfold':\n",
    "            probs_out = pd.DataFrame(data=all_probs,columns = labelnames)\n",
    "        probs_out.to_csv (r'pred_probs.csv', index = False, header=True)\n",
    "        \n",
    "    return model, all_probs, all_y, FPRs, TPRs, AUCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to load data consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data (data_dir = os.getcwd(), features = None, threshold = 100):\n",
    "    file = pd.read_csv(data_dir)\n",
    "    columns = file.columns.values\n",
    "    for i in range (1,len(columns)):\n",
    "        try:\n",
    "            float(columns[i])\n",
    "            n = i\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    Data = file.iloc[:,n:]\n",
    "    \n",
    "    # clean any 'n/a' in data\n",
    "    mask = Data.isnull()\n",
    "    Data = Data[~mask]\n",
    "    \n",
    "    mz = Data.columns.values \n",
    "    mz = mz.astype(float)\n",
    "    if features is not None:\n",
    "        print('test data truncated to include only selected spectral features within (%0.2f) ppm' % threshold)\n",
    "#         indices = np.zeros((1,len(features)))\n",
    "        indices = [None]*len(features)\n",
    "        for j in range (0,len(features)):\n",
    "#             print(j)\n",
    "            index = np.argmin(abs(mz-features[j]))\n",
    "            if (mz[index]-features[j])/features[j]*(10^6) <= threshold:\n",
    "                indices[j] = (index)\n",
    "        mz = mz[indices]\n",
    "        new_indices = [x+n for x in indices]\n",
    "        new_indices = list(range(n))+new_indices\n",
    "        file = file.iloc[:,new_indices]\n",
    "        Data = Data.iloc[:,indices]\n",
    "    else:\n",
    "        print('data already has the same spectral variables')\n",
    "        \n",
    "    return file, Data, mz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions for LR feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_importance (X, Y, num_features, save = True):\n",
    "#     model = ((LogisticRegression(solver='liblinear',class_weight = weights)))\n",
    "    model.fit(X, Y)\n",
    "    importance_LR=np.abs(model.coef_)\n",
    "    size = importance_LR.shape\n",
    "    importance_LR=np.reshape(importance_LR,(size[0], size[1]))\n",
    "    \n",
    "    LR_features = []\n",
    "    for i in range(size[0]):\n",
    "        idx_features = (-importance_LR[i,:]).argsort()[:num_features]\n",
    "        LR_features.append(mz[idx_features])\n",
    "    LR_features=np.reshape(LR_features,(num_features, size[0]))\n",
    "    try:\n",
    "        features_out=pd.DataFrame(LR_features, columns = model.classes_)\n",
    "    except:\n",
    "        features_out=pd.DataFrame(LR_features)\n",
    "#     print(model.classes_)\n",
    "    \n",
    "    if save == True:\n",
    "        features_out.to_csv (r'LR_features.csv', index = False, header=True)\n",
    "    return features_out, importance_LR\n",
    "\n",
    "def deviance(y, yp):\n",
    "    return 2*log_loss(y, yp, normalize=False)\n",
    "\n",
    "def LR_loglike(X,Y, threshold = 1.5, save = True):\n",
    "    GOF=np.zeros((X.shape[1],1))\n",
    "    chi2_pvals=np.zeros((X.shape[1],1))\n",
    "    for p in range(X.shape[1]):\n",
    "        yp=model.fit(X,Y).predict_proba(X)\n",
    "        yp_mean=np.zeros((len(X),2))\n",
    "        for i in range (0,len(X)):\n",
    "            yp_mean[i,0]=yp[:,0].mean()\n",
    "            yp_mean[i,1]=yp[:,1].mean()\n",
    "        res_deviance=deviance(Y,yp)\n",
    "        null_deviance=deviance(Y,yp_mean)\n",
    "        GOF[p]=null_deviance-res_deviance\n",
    "    \n",
    "    chi2_pvals=1-chi2.cdf(GOF , df=1)\n",
    "#     plt.plot(mz,-np.log10(chi2_pvals))\n",
    "\n",
    "    idx_features = (-np.log10(chi2_pvals) > threshold)\n",
    "    mz_dum=mz.reshape(-1,1)\n",
    "    LR_features2=mz_dum[idx_features]\n",
    "    \n",
    "    features_out=pd.DataFrame(LR_features2)\n",
    "    if save == True:\n",
    "        features_out.to_csv (r'LR_features2.csv', index = False, header=True)\n",
    "    return features_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions for RFE feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFE_select (X, Y, n_features, iterate = 1, save = True):\n",
    "    RFE_features = []\n",
    "    start_time = time.time()\n",
    "    for i in range (0,iterate):\n",
    "#         print(i)\n",
    "        selector = RFE(model, n_features_to_select=n_features, step=1)\n",
    "        selector = selector.fit(X, Y)\n",
    "        RFE_features.append(selector.support_)\n",
    "        \n",
    "#     for i, ite in enumerate(RFE_features):\n",
    "#         Data_r3 = X\n",
    "#         Data_r3 = Data_r3.T\n",
    "#         mask = ite\n",
    "#         Data_r3 = Data_r3[mask]\n",
    "#         Data_r3 = Data_r3.T\n",
    "    features_out = mz[tuple(RFE_features)]\n",
    "    features_out=pd.DataFrame(features_out)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print()\n",
    "    print('that took '+str(elapsed_time)+' seconds')\n",
    "    if save == True:\n",
    "        features_out.to_csv (r'RFE_features1.csv', index = False, header=True)\n",
    "    return features_out\n",
    "\n",
    "def RFECV_select (X, Y, iterate = 1, save = True):\n",
    "    RFECV_features = []\n",
    "    start_time = time.time()\n",
    "    for i in range (0,iterate):\n",
    "#         print(i)\n",
    "        selector = RFECV(model, step=1, cv = 5, scoring = 'f1_weighted')\n",
    "        selector = selector.fit(X,Y)\n",
    "        RFECV_features.append(selector.support_)\n",
    "        \n",
    "    features_out = mz[tuple(RFECV_features)]\n",
    "    features_out=pd.DataFrame(features_out)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print()\n",
    "    print('that took '+str(elapsed_time)+' seconds')\n",
    "    if save == True:\n",
    "        features_out.to_csv (r'RFE_features2.csv', index = False, header=True)\n",
    "    return features_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function for LASSO classification & feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LASSO_select(X,Y, max_iter = 10000, tol = 1e-3, n_alphas = 1000, cv=10, save=True):\n",
    "    target = label_binarize(Y,classes=labelnames)\n",
    "\n",
    "    lasso=LassoCV(max_iter=max_iter,tol=tol,n_alphas=n_alphas,cv=cv)\n",
    "    # lasso = Lasso(max_iter=2000,tol=1e-3,)\n",
    "\n",
    "    clf2=lasso.fit(X,target)\n",
    "    print('the LASSO classification accuracy was ' +str(clf2.score(X,target)))\n",
    "    \n",
    "    importance = np.abs(clf2.coef_)\n",
    "    selection=(importance != 0)\n",
    "    lasso_features=mz[selection]\n",
    "    \n",
    "    features_out=pd.DataFrame(lasso_features)\n",
    "    if save == True:\n",
    "        features_out.to_csv (r'LASSO_features.csv', index = False, header=True)\n",
    "    return features_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to find overlapping features from different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(list_of_features,ppm_thresh = 100):\n",
    "    for n, features in enumerate(list_of_features):\n",
    "#         print(n)\n",
    "        if n == 0:\n",
    "            if isinstance(features, pd.DataFrame):\n",
    "                features_matched = features.values\n",
    "            else:\n",
    "                features_matched = features\n",
    "        else:\n",
    "            for feature in features:\n",
    "                index= np.argmin(abs(features_matched-float(feature)))\n",
    "                if (features_matched[index]-feature)/feature*(10^6) > 100:\n",
    "                    np.delete(features_matched, index)\n",
    "    return features_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot (features,mz,box_data,data_label, hue_in = 'no', save = False, savepath = None, swarmyes = True):\n",
    "    group_label = list(box_data.columns)\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.set_context(\"poster\")\n",
    "#     for i in range(group_label):\n",
    "    #         colors = {\"APC_control\": \"g\", \"APC_sim\": \"g\",\"APC_KRAS_control\": \"r\", \"APC_KRAS_sim\": \"r\",\n",
    "#                  \"APC_P53_control\": \"y\", \"APC_P53_sim\": \"y\",\"APC_KRAS_P53_control\": \"b\", \"APC_KRAS_P53_sim\": \"b\"}\n",
    "    for feature in features:\n",
    "        index= np.argmin(abs(mz-float(feature)))\n",
    "        fig = plt.figure(figsize=(15,8))\n",
    "        offset = len(box_data.columns)-len(mz)\n",
    "        if box_data.shape[0]>10000:\n",
    "            print('data too large! plotting using randomised 30% data points.')\n",
    "            sample = np.ceil(box_data.shape[0]/3)\n",
    "            sample = int(sample)\n",
    "            box_data = box_data.sample(n=sample, random_state=1)\n",
    "            swarmyes = False\n",
    "        if hue_in == 'no':\n",
    "            sns.boxplot(x=data_label, y=box_data.columns[index+offset], data=box_data, whis=5, palette = 'bright')\n",
    "            if swarmyes:\n",
    "                sns.swarmplot(x=data_label, y=box_data.columns[index+offset], data=box_data)\n",
    "        else:\n",
    "            sns.boxplot(x=data_label, y=box_data.columns[index+offset], hue=hue_in, data=box_data, whis=5, palette = 'bright')\n",
    "            if swarmyes:\n",
    "                sns.swarmplot(x=data_label, y=box_data.columns[index+offset], hue=hue_in, data=box_data)\n",
    "        if save == True:\n",
    "            filename = 'BP_'+box_data.columns[index+offset]\n",
    "            path = os.path.join(savepath, 'box_plots')\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            os.chdir(path)\n",
    "            fig.savefig(filename+'.png')\n",
    "            plt.close()\n",
    "            print('plot for '+str(feature)+' saved.')\n",
    "        else:\n",
    "            plt.show()\n",
    "    os.chdir('..')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
